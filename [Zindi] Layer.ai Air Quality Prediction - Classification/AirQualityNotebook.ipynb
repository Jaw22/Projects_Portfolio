{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433cc9be",
   "metadata": {
    "id": "433cc9be"
   },
   "source": [
    "# Layer.ai Air Quality Prediction Challenge\n",
    "# By Mohamed Eltayeb & Azer Ksouri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d141a81",
   "metadata": {
    "id": "4d141a81"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91517d92",
   "metadata": {
    "id": "91517d92"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559ab45",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65672b87",
   "metadata": {
    "id": "65672b87"
   },
   "outputs": [],
   "source": [
    "#Plot the LGBM Features Importances\n",
    "def plotImp(model, X , num = 20, fig_size = (40, 20)):\n",
    "    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n",
    "    plt.figure(figsize=fig_size)\n",
    "    sns.set(font_scale = 5)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:num])\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances-01.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoder\n",
    "def label_enc(train_df, test_df, features):\n",
    "    lbl_enc = LabelEncoder()\n",
    "    full_data = pd.concat([train_df[features], test_df[features]],axis=0)\n",
    "    for col in (features):\n",
    "        print(col)\n",
    "        lbl_enc.fit(full_data[col].values)\n",
    "        train_df[col] = lbl_enc.transform(train_df[col])\n",
    "        test_df[col] = lbl_enc.transform(test_df[col])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c11da",
   "metadata": {
    "id": "a49c11da"
   },
   "source": [
    "# Read the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd163a",
   "metadata": {
    "id": "addd163a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be527b52",
   "metadata": {
    "id": "be527b52"
   },
   "source": [
    "# Add The Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386282b",
   "metadata": {
    "id": "4386282b"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(['date','device']).reset_index(drop=True) \n",
    "test_df = test_df.sort_values(['date','device']).reset_index(drop=True)\n",
    "\n",
    "for dataset in (train_df,test_df):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "    dataset['Day'] = dataset.date.dt.day\n",
    "    dataset['Month'] = dataset.date.dt.month\n",
    "    dataset['Year'] = dataset.date.dt.year\n",
    "    dataset['DayOfWeek'] = dataset.date.dt.dayofweek\n",
    "    dataset['DayOfYear'] = dataset.date.dt.dayofyear\n",
    "    dataset['Week'] = dataset.date.dt.weekofyear\n",
    "    dataset.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f92c3a",
   "metadata": {
    "id": "c5f92c3a"
   },
   "outputs": [],
   "source": [
    "ID = test_df['ID']\n",
    "test_df.drop('ID',inplace=True,axis=1)\n",
    "train_df.drop('ID',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53857f36",
   "metadata": {
    "id": "53857f36"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfef88f",
   "metadata": {
    "id": "dbfef88f",
    "outputId": "e0e7bc33-8391-4b5f-b7f4-75d6a0237ddd"
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6a206",
   "metadata": {
    "id": "5ab6a206"
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead70d8",
   "metadata": {
    "id": "4ead70d8",
    "outputId": "20e6d543-56ee-43c6-9cd0-ce4bd8db50ab"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee4bed",
   "metadata": {
    "id": "39ee4bed",
    "outputId": "6cf569a6-a028-4e60-b6ae-c2840bcfd556"
   },
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a57f21",
   "metadata": {
    "id": "97a57f21",
    "outputId": "4a9cf7de-c6a0-4569-a48b-075f4fa7ecfc"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06d557",
   "metadata": {
    "id": "9d06d557"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d4e82",
   "metadata": {
    "id": "635d4e82",
    "outputId": "ea118754-1ab3-4fec-bb9c-89c4d22517be"
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc9336",
   "metadata": {
    "id": "babc9336",
    "outputId": "416d9192-a7f2-49cd-f39c-643927073cbd"
   },
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6d278",
   "metadata": {
    "id": "dfb6d278"
   },
   "outputs": [],
   "source": [
    "#The cardinality of each catgorical feature (Training)\n",
    "cat_cols = train_df.columns\n",
    "for col in cat_cols:\n",
    "    print(col, train_df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070c323",
   "metadata": {
    "id": "2070c323"
   },
   "outputs": [],
   "source": [
    "#The cardinality of each catgorical feature (Testing)\n",
    "cat_cols = test_df.columns\n",
    "for col in cat_cols:\n",
    "    print(col, test_df[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7b047",
   "metadata": {
    "id": "e0f7b047"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d49c54",
   "metadata": {
    "id": "a5d49c54"
   },
   "source": [
    "# Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8f29a",
   "metadata": {
    "id": "20c8f29a"
   },
   "outputs": [],
   "source": [
    "#missing data percentage (Training)\n",
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2dd0c",
   "metadata": {
    "id": "35c2dd0c"
   },
   "outputs": [],
   "source": [
    "#missing data percentage (Testing)\n",
    "total = test_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = test_df.isnull().sum()/test_df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783ebdf",
   "metadata": {
    "id": "9783ebdf"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.bfill().ffill()\n",
    "test_df = test_df.bfill().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f685bf6",
   "metadata": {
    "id": "9f685bf6"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4c4a2",
   "metadata": {
    "id": "28f4c4a2"
   },
   "source": [
    "## - Lags Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37916f9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37916f9d",
    "outputId": "56d9bb11-ee3a-415a-af53-d23e45616c26"
   },
   "outputs": [],
   "source": [
    "def LAG(data,LagFeature,shift=1,NewFeatures=[]) :\n",
    "    data[NewFeatures[0]]   = data[LagFeature]  - data[LagFeature].shift(shift)\n",
    "    data[NewFeatures[1]]   = data[LagFeature].shift(shift)\n",
    "\n",
    "num_feats = train_df.columns\n",
    "num_feats = num_feats.drop(['Week','DayOfYear','DayOfWeek','Year','Month','Day','pm2_5','temp_mean','humidity','site_longitude','site_latitude','device'])\n",
    "\n",
    "for feature in num_feats:\n",
    "    LAG(train_df,LagFeature=f'{feature}',shift=1,NewFeatures=[f'{feature}_diff_Lag1',f'{feature}_Lag1'])\n",
    "    LAG(test_df,LagFeature=f'{feature}',shift=1,NewFeatures=[f'{feature}_diff_Lag1',f'{feature}_Lag1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64caeb",
   "metadata": {
    "id": "4b64caeb"
   },
   "source": [
    "## - Combination Between Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba07fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cba07fe",
    "outputId": "c3e3580a-043a-4973-88c5-3d89304f8b99"
   },
   "outputs": [],
   "source": [
    "for dataset in (train_df,test_df):\n",
    "    dataset['Year_Month'] = dataset['Year'].astype(str) + '-' + dataset['Month'].astype(str)\n",
    "    dataset['Year_Week'] = dataset['Year'].astype(str) + '-' + dataset['Week'].astype(str)\n",
    "    dataset['Year_Month_Day'] = dataset['Year'].astype(str) + '-' + dataset['Month'].astype(str) + '-' + dataset['Day'].astype(str)\n",
    "    \n",
    "feats = ['Year_Month','Year_Week','Year_Month_Day']\n",
    "train_df,test_df = label_enc(train_df,test_df,feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099d339",
   "metadata": {
    "id": "6099d339"
   },
   "source": [
    "## - Aggregations Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba255c96",
   "metadata": {
    "id": "ba255c96"
   },
   "outputs": [],
   "source": [
    "DevicePM2_5Mean = dict(train_df.groupby('device')['pm2_5'].mean())\n",
    "DevicePM2_5Std = dict(train_df.groupby('device')['pm2_5'].std())\n",
    "DevicePM2_5Min = dict(train_df.groupby('device')['pm2_5'].min())\n",
    "DevicePM2_5Max = dict(train_df.groupby('device')['pm2_5'].max())\n",
    "\n",
    "for dataset in (train_df,test_df):\n",
    "    dataset['DevicePM2_5Mean'] = dataset['device'].map(DevicePM2_5Mean)\n",
    "    dataset['DevicePM2_5Std'] = dataset['device'].map(DevicePM2_5Std)\n",
    "    dataset['DevicePM2_5Min'] = dataset['device'].map(DevicePM2_5Min)\n",
    "    dataset['DevicePM2_5Max'] = dataset['device'].map(DevicePM2_5Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2683617",
   "metadata": {
    "id": "e2683617"
   },
   "outputs": [],
   "source": [
    "def Agg(Feature):\n",
    "    for dataset in (train_df,test_df):\n",
    "        dataset[f'{Feature}PerMonth'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].mean()))\n",
    "        dataset[f'{Feature}PerWeek'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].mean()))\n",
    "        dataset[f'{Feature}PerDay'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].mean()))\n",
    "        \n",
    "        dataset[f'{Feature}Month_std'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].std()))\n",
    "        dataset[f'{Feature}Week_std'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].std()))\n",
    "        dataset[f'{Feature}Day_std'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].std()))\n",
    "        \n",
    "        dataset[f'{Feature}Month_min'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].min()))\n",
    "        dataset[f'{Feature}Week_min'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].min()))\n",
    "        dataset[f'{Feature}Day_min'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].min()))\n",
    "       \n",
    "        dataset[f'{Feature}Month_max'] = dataset['Month'].map(dict(dataset.groupby('Month')[Feature].max()))\n",
    "        dataset[f'{Feature}Week_max'] = dataset['Year_Week'].map(dict(dataset.groupby('Year_Week')[Feature].max()))\n",
    "        dataset[f'{Feature}Day_max'] = dataset['Year_Month_Day'].map(dict(dataset.groupby('Year_Month_Day')[Feature].max()))\n",
    "        \n",
    "Agg('temp_mean')\n",
    "Agg('humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd36e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['Year_Month','Year_Week','Year_Month_Day'],inplace=True,axis=1)\n",
    "test_df.drop(['Year_Month','Year_Week','Year_Month_Day'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a943e",
   "metadata": {
    "id": "ff0a943e"
   },
   "source": [
    "## - Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1a9ec",
   "metadata": {
    "id": "17e1a9ec"
   },
   "outputs": [],
   "source": [
    "def Rolling(feature):\n",
    "    for dataset in (train_df,test_df):\n",
    "        dataset[f'{feature}_Rolling_3'] = dataset[feature].rolling(3).mean()\n",
    "        dataset[f'{feature}_Rolling_5'] = dataset[feature].rolling(5).mean()\n",
    "\n",
    "        dataset[f\"{feature}_rolling_mean_60\"] = dataset.rolling(60).mean()[feature]\n",
    "        dataset[f\"{feature}_rolling_max_60\"] = dataset.rolling(60).max()[feature]\n",
    "        dataset[f\"{feature}_rolling_min_60\"] = dataset.rolling(60).min()[feature]\n",
    "\n",
    "        dataset[f\"{feature}_rolling_mean_30\"] = dataset.rolling(30).mean()[feature]\n",
    "        dataset[f\"{feature}_rolling_max_30\"] = dataset.rolling(30).max()[feature]\n",
    "        dataset[f\"{feature}_rolling_min_30\"] = dataset.rolling(30).min()[feature]\n",
    "\n",
    "        dataset[f\"{feature}_rolling_mean_10\"] = dataset.rolling(10).mean()[feature]\n",
    "        dataset[f\"{feature}_rolling_max_10\"] = dataset.rolling(10).max()[feature]\n",
    "        dataset[f\"{feature}_rolling_min_10\"] = dataset.rolling(10).min()[feature]\n",
    "\n",
    "Rolling('temp_mean')\n",
    "Rolling('humidity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2c5fd",
   "metadata": {
    "id": "a6f2c5fd"
   },
   "source": [
    "## - Polar Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d7d72",
   "metadata": {
    "id": "8b6d7d72"
   },
   "outputs": [],
   "source": [
    "def Polar(X,y, a = 0, b = 0): # a and b represnt the center\n",
    "    r = np.sqrt((X-a)**2 + (y-b)**2)\n",
    "    phi = np.arctan2((y-a), (X-b))\n",
    "    return r, phi\n",
    "\n",
    "train_df['R'], train_df['Phi'] = Polar(train_df[\"site_latitude\"],train_df[\"site_longitude\"])\n",
    "test_df['R'], test_df['Phi'] = Polar(test_df[\"site_latitude\"],test_df[\"site_longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bd15d",
   "metadata": {
    "id": "e94bd15d"
   },
   "source": [
    "## - Foureier Frequnecies and Amplitudes For Features That Contain Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc6e8b",
   "metadata": {
    "id": "c3fc6e8b"
   },
   "outputs": [],
   "source": [
    "freq2_dict_no_log = dict()\n",
    "freq3_dict_no_log = dict()\n",
    "\n",
    "amp2_dict_no_log = dict()\n",
    "amp3_dict_no_log = dict()\n",
    "\n",
    "for feat_1 in ('Year','Month','Day'):\n",
    "    for feat_2 in ('temp_mean', 'humidity'):\n",
    "        for i in range(min(train_df[feat_1].unique()), max(train_df[feat_1].unique()) + 1):\n",
    "\n",
    "            a = train_df.loc[train_df[feat_1]==i]\n",
    "            a_sales = a[feat_2]\n",
    "\n",
    "            Y = np.fft.fft(a_sales.values)\n",
    "            Y = abs(Y)\n",
    "            freq = np.fft.fftfreq(len(Y), 1)\n",
    "\n",
    "            intercept_index = np.argmax(Y)\n",
    "            Y = np.delete(Y, intercept_index)\n",
    "            freq = np.delete(freq, intercept_index)\n",
    "\n",
    "            amplitude_1_index = np.argmax(Y)\n",
    "            amplitude_1 = Y[amplitude_1_index]\n",
    "            Y = np.delete(Y, amplitude_1_index)\n",
    "            freq_1 = freq[amplitude_1_index]\n",
    "            freq = np.delete(freq, amplitude_1_index)\n",
    "\n",
    "            amplitude_2_index = np.argmax(Y)\n",
    "            amplitude_2 = Y[amplitude_2_index]\n",
    "            Y = np.delete(Y, amplitude_2_index)\n",
    "            freq_2 = freq[amplitude_2_index]\n",
    "            freq = np.delete(freq, amplitude_2_index)\n",
    "\n",
    "            amplitude_3_index = np.argmax(Y)\n",
    "            amplitude_3 = Y[amplitude_3_index]\n",
    "            Y = np.delete(Y, amplitude_3_index)\n",
    "            freq_3 = freq[amplitude_3_index]\n",
    "            freq = np.delete(freq, amplitude_3_index)\n",
    "\n",
    "            #Freq_1 is not included because it seems as it is always 0\n",
    "            a[f'Frequency_2_{feat_1}_{feat_2}'] = freq_2\n",
    "            a[f'Frequency_3_{feat_1}_{feat_2}'] = freq_3\n",
    "\n",
    "            a[f'Amplitude_2_{feat_1}_{feat_2}'] = amplitude_2\n",
    "            a[f'Amplitude_3_{feat_1}_{feat_2}'] = amplitude_3\n",
    "\n",
    "\n",
    "            freq2_dict_no_log[i] = freq_2\n",
    "            freq3_dict_no_log[i] = freq_3\n",
    "\n",
    "            amp2_dict_no_log[i] = amplitude_2\n",
    "            amp3_dict_no_log[i] = amplitude_3\n",
    "\n",
    "\n",
    "            if i == min(train_df[feat_1].unique()):\n",
    "                k = a\n",
    "            else:\n",
    "                k = pd.concat([k,a])\n",
    "                \n",
    "        train_df = k\n",
    "        \n",
    "        test_df[f'Frequency_2_{feat_1}_{feat_2}'] = test_df[feat_1].map(freq2_dict_no_log)\n",
    "        test_df[f'Frequency_3_{feat_1}_{feat_2}'] = test_df[feat_1].map(freq3_dict_no_log)\n",
    "        test_df[f'Amplitude_2_{feat_1}_{feat_2}'] = test_df[feat_1].map(amp2_dict_no_log)\n",
    "        test_df[f'Amplitude_3_{feat_1}_{feat_2}'] = test_df[feat_1].map(amp3_dict_no_log)\n",
    "        \n",
    "        freq2_dict_no_log = dict()\n",
    "        freq3_dict_no_log = dict()\n",
    "        amp2_dict_no_log = dict()\n",
    "        amp3_dict_no_log = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87318634",
   "metadata": {
    "id": "87318634"
   },
   "source": [
    "## - Percentage change in Temperature and Humidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61001b",
   "metadata": {
    "id": "aa61001b"
   },
   "outputs": [],
   "source": [
    "periods = [1, 3, 7, 14]\n",
    "for period in periods:\n",
    "    train_df.loc[:, f\"PctChangeTemp_{period}\"] = train_df[\"temp_mean\"].pct_change(period)\n",
    "    train_df.loc[:, f\"PctChangeHumi_{period}\"] = train_df[\"humidity\"].pct_change(period)\n",
    "    test_df.loc[:, f\"PctChangeTemp_{period}\"] = test_df[\"temp_mean\"].pct_change(period)\n",
    "    test_df.loc[:, f\"PctChangeHumi_{period}\"] = test_df[\"humidity\"].pct_change(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559cc2c",
   "metadata": {
    "id": "4559cc2c"
   },
   "source": [
    "## - Historic Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e95e9",
   "metadata": {
    "id": "389e95e9"
   },
   "outputs": [],
   "source": [
    "periods = [3, 7, 14]\n",
    "for period in periods:\n",
    "    train_df.loc[:, f\"volatility_temp_mean_{period}\"] = train_df[\"temp_mean\"].diff().rolling(period).std()\n",
    "    test_df.loc[:, f\"volatility_temp_mean_{period}\"] = test_df[\"temp_mean\"].diff().rolling(period).std()\n",
    "    train_df.loc[:, f\"volatility_humidity_{period}\"] = train_df[\"humidity\"].diff().rolling(period).std()\n",
    "    test_df.loc[:, f\"volatility_humidity_{period}\"] = test_df[\"humidity\"].diff().rolling(period).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c797fd7",
   "metadata": {
    "id": "5c797fd7"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78094ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d78094ec",
    "outputId": "b8cce322-8932-45e4-f12f-bbfc2b782f7c"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = label_enc(train_df,test_df,['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52efdb8",
   "metadata": {
    "id": "e52efdb8"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13d03f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc13d03f",
    "outputId": "a1951984-d2c3-4670-c4b6-98d72ee2a5ec"
   },
   "outputs": [],
   "source": [
    "#Averaging the predictions of the same model with different seeds to get more consistent results\n",
    "X = train_df.drop('pm2_5',axis=1)\n",
    "y = train_df['pm2_5']\n",
    "\n",
    "Predictions = pd.DataFrame()\n",
    "\n",
    "for seed in range(20,46):\n",
    "    print(f'Seed: {seed}')\n",
    "    params = {'n_estimators': 2064, 'learning_rate': 0.03196897706232692, 'depth': 5, 'reg_lambda': 12.680808984686983}\n",
    "    CB = CatBoostRegressor(**params,verbose=0, random_state=seed, task_type = 'GPU')\n",
    "    LogCB = TransformedTargetRegressor(CB, func = np.log1p, inverse_func = np.expm1)  \n",
    "    LogCB.fit(X, y)\n",
    "\n",
    "    Predictions[f'Target_{seed}'] = LogCB.predict(test_df)\n",
    "    Predictions[f'Target_{seed}'] = Predictions[f'Target_{seed}'] * 0.975  #A Correction Factor of 0.975\n",
    "    \n",
    "#Averaging the Results\n",
    "Predictions['Mean'] = Predictions.mean(axis=1)\n",
    "Predictions['HMean'] = Predictions.apply(stats.hmean, axis=1)\n",
    "Predictions['GMean'] = Predictions.apply(stats.gmean, axis=1)\n",
    "\n",
    "#Averaging the Second Results\n",
    "FinalPred = Predictions[['Mean','HMean','GMean']].apply(stats.hmean,axis=1)\n",
    "\n",
    "#Making the submission file\n",
    "submission = pd.DataFrame({\"Id\": ID ,\"pm2_5\": FinalPred.values})\n",
    "submission.to_csv('AirQualityPrediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2cc4e",
   "metadata": {
    "id": "fbd2cc4e"
   },
   "source": [
    "## Show the Features Importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c704b",
   "metadata": {
    "id": "f07c704b",
    "outputId": "744d1997-cc5d-4c1b-b78d-ba43e1fb6f22"
   },
   "outputs": [],
   "source": [
    "CB.fit(X,y)\n",
    "plotImp(CB,X)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Best 1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
